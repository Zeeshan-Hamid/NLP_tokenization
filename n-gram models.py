# -*- coding: utf-8 -*-
"""NLP Question 5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10tg5RvwpxBC4IYsC4LndE6MC-GBewGCH

# Question 5 (tokenizer)
"""

!pip install sentencepiece tokenizers

import pandas as pd
# source is "https://medium.com/codex/sentencepiece-a-simple-and-language-independent-subword-tokenizer-and-detokenizer-for-neural-text-ffda431e704e"
import sentencepiece as spm
# i am using hugging face here
from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import Whitespace

file_path = 'data.csv'
df = pd.read_csv(file_path, encoding='utf-8')

print("data loaded succesfully")
print(df.head())


#to do sentence tokenization using the sentence piece we need to store it in a temporary file seperate
df['Text'].to_csv('urdu_text.txt', index=False, header=False)

"""#### Subword tokenization"""

# Training Sentence piece
spm.SentencePieceTrainer.Train('--input=urdu_text.txt --model_prefix=urdu_sp --vocab_size=5000 --character_coverage=0.9995 --model_type=bpe')

sp = spm.SentencePieceProcessor()
sp.load('urdu_sp.model')

# subword tokeization
df['Subword_Tokenized'] = df['Text'].apply(lambda x: sp.encode_as_pieces(x))
print(df[['Text', 'Subword_Tokenized']].head())

"""#### Byte pair encoding"""

tokenizer = Tokenizer(BPE())
trainer = BpeTrainer(vocab_size=5000, special_tokens=["<pad>", "<s>", "</s>", "<unk>", "<mask>"]) #these are most commonly used that I found on internet

# Use whitespace pre-tokenizer for initial word boundaries
tokenizer.pre_tokenizer = Whitespace()

# Training BPE tokenizer
tokenizer.train(["urdu_text.txt"], trainer)

# apply BPE tokenization
def apply_bpe_tokenizer(text):
    encoded = tokenizer.encode(text)
    return encoded.tokens


df['BPE_Tokenized'] = df['Text'].apply(apply_bpe_tokenizer)

print("\nBPE tokenization applied successfully:")
print(df[['Text', 'BPE_Tokenized']].head())

"""#### MaxMatch Segmentation"""

# Implementing the MaxMatch manually
class MaxMatchTokenizer:
    def __init__(self, vocab):
        self.vocab = set(vocab)

    def tokenize(self, text):
        tokens = []
        while text:
            for i in range(len(text), 0, -1):
                if text[:i] in self.vocab:
                    tokens.append(text[:i])
                    text = text[i:]
                    break
            else:
                # appending  first char as a token if nothing is matched
                tokens.append(text[0])
                text = text[1:]
        return tokens

# Building vocabulary
vocab = set(" ".join(df['Text']).split())


maxmatch_tokenizer = MaxMatchTokenizer(vocab)


df['MaxMatch_Tokenized'] = df['Text'].apply(lambda x: maxmatch_tokenizer.tokenize(x))

print("\nMaxMatch tokenization applied successfully:")
print(df[['Text', 'MaxMatch_Tokenized']].head())

print("\nComparison of Tokenization Methods:")
print(df[['Text', 'Subword_Tokenized', 'BPE_Tokenized', 'MaxMatch_Tokenized']].head())

"""#### Analysis of tokenization performance"""

# calulating the average length
df['Subword_Length'] = df['Subword_Tokenized'].apply(len)
df['BPE_Length'] = df['BPE_Tokenized'].apply(len)
df['MaxMatch_Length'] = df['MaxMatch_Tokenized'].apply(len)

# average token length
avg_subword_len = df['Subword_Length'].mean()
avg_bpe_len = df['BPE_Length'].mean()
avg_maxmatch_len = df['MaxMatch_Length'].mean()

print(f"Average token length (Subword Tokenization): {avg_subword_len}")
print(f"Average token length (BPE): {avg_bpe_len}")
print(f"Average token length (MaxMatch): {avg_maxmatch_len}")

# Check for rare or unseen words
# In MaxMatch a long word will be broken into many single-character tokens if not found in the vocabulary
df['MaxMatch_Unseen_Words'] = df['MaxMatch_Tokenized'].apply(lambda tokens: [t for t in tokens if len(t) == 1])

# Counting how often unseen tokens appear
unseen_word_count = df['MaxMatch_Unseen_Words'].apply(len).sum()
print(f"Number of single-character tokens (likely unseen words) in MaxMatch: {unseen_word_count}")


print("\nComparison of tokenization results:")

for i in range(5):
    print(f"\nOriginal Text: {df['Text'].iloc[i]}")
    print(f"Subword Tokenized: {df['Subword_Tokenized'].iloc[i]}")
    print(f"BPE Tokenized: {df['BPE_Tokenized'].iloc[i]}")
    print(f"MaxMatch Tokenized: {df['MaxMatch_Tokenized'].iloc[i]}")

"""# Question 5 (perplexity)"""

!pip install nltk

import nltk
from nltk import word_tokenize, bigrams, trigrams
from collections import Counter, defaultdict
import math
import pandas as pd

file_path = 'data.csv'
df = pd.read_csv(file_path, encoding='utf-8')

#Tokenize the text into words using NLTK
nltk.download('punkt')

# creating a single corpus using all the data
corpus = " ".join(df['Text'])

tokens = word_tokenize(corpus)

train_tokens = tokens

sample_texts = [
    "یہی کچھ اس بار بھی ہوا جب دو دنوں کی بارش نے ملک کے اکثر کو بلکہ بڑے بڑے شہروں کو تالاب میں تبدیل کر دیا۔",
    "موسم کی تازہ ترین پیشگوئی کے مطابق آنے والے دنوں میں مزید بارشوں کا امکان ہے۔",
    "تعلیم ایک اہم ذریعہ ہے جس کے ذریعے ہم معاشرتی ترقی اور خوشحالی حاصل کر سکتے ہیں۔",
    "عوامی مسائل کے حل کے لئے حکومت نے متعدد نئے منصوبے شروع کئے ہیں۔",
    "کھیلوں کے مقابلے ملک بھر میں نوجوانوں کے لئے تفریح کا بہترین ذریعہ ہیں۔"
]

# Function to tokenize the sample text
def tokenize_sample(sample):
    return word_tokenize(sample)

"""#### Unigram Model"""

unigram_freq = Counter(train_tokens)

total_words_unigram = sum(unigram_freq.values())


def unigram_prob(word):
    return unigram_freq[word] / total_words_unigram if word in unigram_freq else 1 / (total_words_unigram + len(unigram_freq))

def perplexity_unigram(test_tokens):
    perplexity = 0
    N = len(test_tokens)

    for word in test_tokens:
        prob = unigram_prob(word)
        perplexity += math.log(prob)

    return math.exp(-perplexity / N)

"""#### Bigram Model"""

bigram_freq = defaultdict(lambda: defaultdict(int))

for w1, w2 in bigrams(train_tokens):
    bigram_freq[w1][w2] += 1


def bigram_prob(w1, w2):
    if bigram_freq[w1][w2] == 0:
        return 1 / len(unigram_freq)  # Smoothing for unseen bigrams
    return bigram_freq[w1][w2] / unigram_freq[w1]


def perplexity_bigram(test_tokens):
    perplexity = 0
    N = len(test_tokens) - 1

    for i in range(N):
        w1, w2 = test_tokens[i], test_tokens[i + 1]
        prob = bigram_prob(w1, w2)
        perplexity += math.log(prob)

    return math.exp(-perplexity / N)

"""#### Trigram Model"""

trigram_freq = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))

for w1, w2, w3 in trigrams(train_tokens):
    trigram_freq[w1][w2][w3] += 1


def trigram_prob(w1, w2, w3):
    if trigram_freq[w1][w2][w3] == 0:
        return bigram_prob(w2, w3)  # Backoff to bigram if trigram not found
    return trigram_freq[w1][w2][w3] / bigram_freq[w1][w2]


def perplexity_trigram(test_tokens):
    perplexity = 0
    N = len(test_tokens) - 2

    for i in range(N):
        w1, w2, w3 = test_tokens[i], test_tokens[i + 1], test_tokens[i + 2]
        prob = trigram_prob(w1, w2, w3)
        perplexity += math.log(prob)

    return math.exp(-perplexity / N)

print("Calculating perplexities\n")

for i, sample_text in enumerate(sample_texts):
    print(f"Sample Text {i+1}: {sample_text}\n")
    test_tokens = tokenize_sample(sample_text)


    unigram_perplexity = perplexity_unigram(test_tokens)
    print(f"Unigram Model Perplexity: {unigram_perplexity}")


    bigram_perplexity = perplexity_bigram(test_tokens)
    print(f"Bigram Model Perplexity: {bigram_perplexity}")


    trigram_perplexity = perplexity_trigram(test_tokens)
    print(f"Trigram Model Perplexity: {trigram_perplexity}")

    print("---------------------------------------------------")


print("\nThe end")